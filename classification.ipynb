{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm,trange\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = np.load('normal_adjacent.npy')\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"compute L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "    adj = sp.coo_matrix(adj, dtype=np.float32)\n",
    "    adj += sp.eye(adj.shape[0])\n",
    "    degree = np.array(adj.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    norm_adj = d_hat.dot(adj).dot(d_hat)\n",
    "    return sparse_mx_to_torch_sparse_tensor(norm_adj)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "adj0 = sp.coo_matrix(AA, dtype=np.float32)\n",
    "adj0 = adj0 + adj0.T.multiply(adj0.T > adj0) - adj0.multiply(adj0.T > adj0)\n",
    "adj0 = sparse_mx_to_torch_sparse_tensor((adj0 + sp.eye(adj0.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_tensor_in = np.load('taxi_tensor_in.npy')\n",
    "taxi_tensor_in1 = np.load('taxi_tensor_in.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    " \n",
    "def get_weekday(date):\n",
    "    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    # 将输入的日期字符串转换为datetimemd\n",
    "    date_obj = datetime.datetime.strptime(date, '%m/%d/%Y')\n",
    "    \n",
    "    # 获取该日期所在的星期几（0表示星期一）\n",
    "    weekday_index = date_obj.weekday()\n",
    "    \n",
    "    return weekdays[weekday_index]\n",
    "\n",
    "import holidays\n",
    "\n",
    "us_holidays = holidays.US()\n",
    "def holiday_(date):\n",
    "    if date in us_holidays:\n",
    "        return ', holiday'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = np.load('text1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4344/4344 [00:00<00:00, 7423.33it/s]\n"
     ]
    }
   ],
   "source": [
    "A_com = np.load(r'normal_adjacent.npy')\n",
    "connections = []\n",
    "for i in range(78):\n",
    "    for j in range(i, 78):\n",
    "        if A_com[i][j] == 1:\n",
    "            connections.append('(r'+str(i)+', r' + str(j)+')')\n",
    "            \n",
    "xx = ''\n",
    "for i in range(len(connections)):\n",
    "    xx = xx + connections[i]+','\n",
    "\n",
    "regions  = 'r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15, r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31, r32, r33, r34, r35, r36, r37, r39, r39, r40, r41, r42, r43, r44, r45 r46, r47, r48, r49, r50, r51, r52, r53, r54, r55, r56, r57, r58, r59, r60, r61, r62, r63, r64, r65, r66, r67, r68, r69, r70, r71, r72, r73, r74, r75, r76, r77'\n",
    "\n",
    "\n",
    "text_s = []\n",
    "for i in tqdm(range(len(taxi_tensor_in))):\n",
    "    text_s.append('At ' + text1[i]\\\n",
    "            +', there were' \\\n",
    "            + ' ' + str(list(map(int, taxi_tensor_in[i])))[1:-1] \\\n",
    "            + ' taxis visiting Community regions ' + regions+'.'\\\n",
    "            + ' The spatial sequence has a minimum of ' \\\n",
    "            + str(int(taxi_tensor_in[i].min())) \\\n",
    "            + ' at region ' + 'r'+ str(list(taxi_tensor_in[i]).index(min(taxi_tensor_in[i])))\\\n",
    "            +'; a second minimum of ' + str(int(sorted(taxi_tensor_in[i], reverse=False)[2]))\\\n",
    "            + ' at region ' + 'r'+ str(list(taxi_tensor_in[i]).index(int(sorted(taxi_tensor_in[i], reverse=False)[2])))\\\n",
    "            +'; a maximum of ' + str(int(taxi_tensor_in[i].max())) \\\n",
    "            +' at region ' + 'r'+ str(list(taxi_tensor_in[i]).index(max(taxi_tensor_in[i])))\\\n",
    "            +'; a second maximum of ' + str(int(sorted(taxi_tensor_in[i], reverse=False)[-2])) \\\n",
    "            +' at region ' + 'r'+ str(list(taxi_tensor_in[i]).index(sorted(taxi_tensor_in[i], reverse=False)[-2]))\\\n",
    "            + '; and a mean of '+str(round(taxi_tensor_in[i].mean(),2))\\\n",
    "            +'. There are spatial correlations between regions. Adjacent regions may affect each other.'\\\n",
    "            +'The regions in parentheses below are adjacent:' + xx[:-1]+'.')\n",
    "    \n",
    "def text_T(T,t,R):\n",
    "    label = 'From ' + text1[T] + ', to ' + text1[T+t-1]\\\n",
    "            +', there were ' + str(list(map(int, taxi_tensor_in[T:T+t,R])))[1:-1]\\\n",
    "            + ' taxis visiting Community region ' + 'r'+str(R)+'.'\\\n",
    "            + ' The time interval is 1 hour.'\\\n",
    "            + ' The temporal sequence has a minimum of '\\\n",
    "            + str(int(taxi_tensor_in[T:T+t,R].min()))\\\n",
    "            + ' at ' + text1[T+list(taxi_tensor_in[T:T+t,R]).index(taxi_tensor_in[T:T+t,R].min())] \\\n",
    "            +'; a second minimum of ' + str(int(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[2]))\\\n",
    "            + ' at '+ text1[T+(list(taxi_tensor_in[T:T+t,R]).index(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[2]))]\\\n",
    "            +'; a maximum of ' + str(int(taxi_tensor_in[T:T+t,R].max()))\\\n",
    "            + ' at ' + text1[T+list(taxi_tensor_in[T:T+t,R]).index(taxi_tensor_in[T:T+t,R].max())] \\\n",
    "            +'; a second maximum of ' + str(int(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[-2])) \\\n",
    "            +' at '+ text1[T+(list(taxi_tensor_in[T:T+t,R]).index(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[-2]))]\\\n",
    "            + '; and a mean of '+str(round(taxi_tensor_in[T:T+t,R].mean(),2))+'.'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "  \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj_):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.spmm(adj_, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc10 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc11 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = 16, eps = 1e-6)\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)\n",
    "        self.Embedding = Embedding()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x1 = self.Embedding(x.reshape([78,1]))\n",
    "        x2 = self.gc11(F.dropout(F.relu(self.gc10(x1, adj)), self.dropout, training = self.training), adj)\n",
    "        return self.dropout_layer(self.layer_norm(x2))\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.linear = nn.Linear(in_features = 1, out_features = 16, bias=True)\n",
    "\n",
    "    def forward(self, X_feature):\n",
    "        X = self.linear(X_feature)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class transformer_t(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model = 16, n_heads = 4, dropout = 0.4):\n",
    "        super(transformer_t, self).__init__()\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, n_heads, dropout = 0.4),\n",
    "                                             num_layers = 1,\n",
    "                                             norm = nn.LayerNorm(normalized_shape = d_model, eps = 1e-6))\n",
    "        self.positional_encoding = PositionalEmbedding(d_model)\n",
    "        self.predictor = nn.Linear(d_model, 8)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Embedding = Embedding()\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.reshape([8,1])\n",
    "        x1 = self.Embedding(src.reshape([8,1]))\n",
    "        out = self.predictor(self.encoder(x1 + self.positional_encoding(src)))\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(8)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        prob = torch.matmul(attn, V)\n",
    "        return prob\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dk):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = 2\n",
    "        self.W_Q = nn.Linear(dk, 8 * 2, bias=False)\n",
    "        self.W_K = nn.Linear(4096, 8 * 2, bias=False)\n",
    "        self.W_V = nn.Linear(4096, 8 * 2, bias=False)\n",
    "        self.fc = nn.Linear(8 * 2, 4096, bias=False)  # ff 全连接\n",
    "        self.layer_norm = nn.LayerNorm(16)  # normal 归一化\n",
    "        self.ScaledDotProductAttention = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V):\n",
    "        # input_Q：1*4*6，每批1句 * 每句4个词 * 每词6长度编码\n",
    "        # residual 先临时保存下：原始值，后面做残差连接加法\n",
    "        residual, batch = input_Q, input_Q.size(0)\n",
    "\n",
    "        # 乘上 W 矩阵。注：W 就是要训练的参数\n",
    "        # 注意：维度从2维变成3维，增加 head 维度，也是一次性并行计算\n",
    "        Q = self.W_Q(input_Q)  # 乘以 W(6*6) 变为 1*4*6\n",
    "        Q = Q.view(batch, -1, 2, 8).transpose(1, 2)  # 切开为2个Head 变为 1*2*4*3 1批 2个Head 4词 3编码\n",
    "        K = self.W_K(input_K).view(batch, -1, 2, 8).transpose(1, 2)\n",
    "        V = self.W_V(input_V).view(batch, -1, 2, 8).transpose(1, 2)\n",
    "\n",
    "        # 返回1*2*4*3，2个头，4*3为带上关注关系的4词\n",
    "        prob = self.ScaledDotProductAttention(Q, K, V)\n",
    "\n",
    "        # 把2头重新拼接起来，变为 1*4*6\n",
    "        prob = prob.transpose(1, 2).contiguous()\n",
    "        prob = prob.view(batch, -1, 2 * 8).contiguous()\n",
    "\n",
    "        # 全连接层：对多头注意力的输出进行线性变换，从而更好地提取信息\n",
    "        output = self.fc(prob)\n",
    "\n",
    "        # 残差连接 & 归一化\n",
    "        # res = self.layer_norm(residual + output) # return 1*4*6\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = ['most', 'second', 'third']\n",
    "\n",
    "def questions_(t0, t1, t2, t3,rank0, rank1):\n",
    "    questions = []\n",
    "    questions.append('Given the historical traffic flows for 78 Community regions from ' + text1[t0] + ' to ' +  text1[t1] +'.' \\\n",
    "                     + ' Your task is to predict the ' + rank[rank0] + ' congested time ' + 'from ' + text1[t2] + ' to ' +  text1[t3] +'.')\n",
    "    \n",
    "    questions.append('Given the historical traffic flows for 78 Community regions from ' + text1[t0] + ' to ' +  text1[t1] +'.' \\\n",
    "                     + ' Your task is to predict the Community region with the ' + rank[rank1] + ' maximum traffic flow'\\\n",
    "                     + ' from ' + text1[t2] + ' to ' +  text1[t3] +'.')\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "def answers_(t0, t1, t2, t3,rank0, rank1):\n",
    "    answers = []\n",
    "    answers.append(text1[t2+np.where(np.sum(taxi_tensor_in1[t2:t3+1], axis=1) \n",
    "                                     == sorted(np.sum(taxi_tensor_in1[t2:t3+1], axis=1), reverse = True)[rank0])[0][0]][0:4])\n",
    "    \n",
    "    answers.append('r'+ str(np.where(np.sum(taxi_tensor_in1[t2:t3+1], axis=0) \n",
    "                                    == sorted(np.sum(taxi_tensor_in1[t2:t3+1], axis=0),reverse = True)[rank1])[0][0]))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.llama_config = LlamaConfig.from_pretrained('Llama-2-7b-hf/')\n",
    "        self.llama_config.num_hidden_layers = 1\n",
    "        self.llama_config.output_attentions = True\n",
    "        self.llama_config.output_hidden_states = True\n",
    "        self.llama = LlamaModel.from_pretrained(\"Llama-2-7b-hf/\",\n",
    "                                                use_safetensors=True)\n",
    "\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(\"Llama-2-7b-hf/\")\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llama.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.MultiHeadAttention0 = MultiHeadAttention(1248)\n",
    "        self.MultiHeadAttention1 = MultiHeadAttention(64)\n",
    "        self.model_graphlearning = GCN(nfeat = 16, nhid = 32, nclass = 16, dropout = 0.4)\n",
    "        self.tt_trans = transformer_t(dropout = 0.4)\n",
    "        self.Softmaxlayer = torch.nn.Softmax(dim = 1)\n",
    "        self.output_projection0 = nn.Linear(4096, 104, bias = True)\n",
    "        self.output_projection1 = nn.Linear(80, 1, bias = True)\n",
    "        # self.output_projection1 = nn.Linear(5504, 104, bias = True)\n",
    "\n",
    "    def text_T(self, T, t, R):\n",
    "        label = 'From ' + text1[T] + ', to ' + text1[T+t-1]\\\n",
    "                +', there were ' + str(list(map(int, taxi_tensor_in[T:T+t,R])))[1:-1]\\\n",
    "                + ' taxis visiting Community region ' + 'r'+str(R)+'.'\\\n",
    "                + ' The time interval is 1 hour.'\\\n",
    "                + ' The temporal sequence has a minimum of '\\\n",
    "                + str(int(taxi_tensor_in[T:T+t,R].min()))\\\n",
    "                + ' at ' + text1[T+list(taxi_tensor_in[T:T+t,R]).index(taxi_tensor_in[T:T+t,R].min())] \\\n",
    "                +'; a second minimum of ' + str(int(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[2]))\\\n",
    "                + ' at '+ text1[T+(list(taxi_tensor_in[T:T+t,R]).index(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[2]))]\\\n",
    "                +'; a maximum of ' + str(int(taxi_tensor_in[T:T+t,R].max()))\\\n",
    "                + ' at ' + text1[T+list(taxi_tensor_in[T:T+t,R]).index(taxi_tensor_in[T:T+t,R].max())] \\\n",
    "                +'; a second maximum of ' + str(int(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[-2])) \\\n",
    "                +' at '+ text1[T+(list(taxi_tensor_in[T:T+t,R]).index(sorted(taxi_tensor_in[T:T+t,R], reverse=False)[-2]))]\\\n",
    "                + '; and a mean of '+str(taxi_tensor_in[T:T+t,R].mean())+'.'\n",
    "        return label\n",
    "\n",
    "    def forward(self, x_input, x_s_text, adj, T, t, question):\n",
    "        token_whole = torch.empty((0)).cuda()\n",
    "        for i in range(t):\n",
    "            prompt = self.tokenizer(x_s_text[T+i], return_tensors = \"pt\", padding = True, truncation = True, \n",
    "                                    max_length = 500).input_ids.cuda()\n",
    "            token_whole = torch.concat([token_whole, \n",
    "                                        self.MultiHeadAttention0(self.model_graphlearning(x_input[T+i], adj).reshape([1, 1,-1]), \n",
    "        self.llama.get_input_embeddings()(prompt)[0],\n",
    "        self.llama.get_input_embeddings()(prompt)[0])], dim=1)\n",
    "            \n",
    "        # token_t = torch.empty((x_input.shape[1]))\n",
    "        for j in range(78):\n",
    "            prompt = self.tokenizer(self.text_T(T,t,j), return_tensors = \"pt\", padding = True, truncation = True, \n",
    "                                    max_length = 500).input_ids.cuda()\n",
    "            token_whole = torch.concat([token_whole, \n",
    "                                        self.MultiHeadAttention1(self.tt_trans(x_input[T:T+t, j]).reshape([1, 1, -1]), \n",
    "                                                                 self.llama.get_input_embeddings()(prompt)[0],\n",
    "                                                                 self.llama.get_input_embeddings()(prompt)[0])],\n",
    "                                       dim=1)\n",
    "        select_question = self.tokenizer(question, return_tensors = \"pt\", padding = True, truncation = True, \n",
    "                                         max_length = 100).input_ids.cuda()\n",
    "        token_whole = torch.concat([token_whole, self.llama.get_input_embeddings()(select_question)], dim = 1)\n",
    "        dec_out = self.llama(inputs_embeds = token_whole).last_hidden_state[:, -80: , :]\n",
    "        dec_out =  self.output_projection1(self.output_projection0(dec_out[0]).T)\n",
    "        \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.33s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Model(nfeat = 16, nhid = 32, nclass = 16, dropout = 0.4)\n",
    "model = model.cuda()\n",
    "taxi_tensor_in = torch.FloatTensor(taxi_tensor_in).cuda()\n",
    "adj0 = adj0.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.0005, weight_decay = 5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    lossall = []\n",
    "    loss_train = 0\n",
    "    for i in random.sample(range(0, 4300), 4):\n",
    "        rank0_select = random.randint(0, 2)\n",
    "        rank1_select = random.randint(0, 2)\n",
    "        question = questions_(i, i+7, i+8, i+15,rank0_select, rank1_select)\n",
    "        for j in range(2):\n",
    "            answer = torch.zeros([1,104])\n",
    "            answer[0, list(answer_list).index(answers_(i, i+7, i+8, i+15,rank0_select, rank1_select)[j])] = 1\n",
    "#             output = model(taxi_tensor_in, text_s, adj0, i, 8, question[j])\n",
    "            loss = criterion(model(taxi_tensor_in, text_s, adj0, i, 8, question[j]).T.cpu(), answer)\n",
    "            loss_train = loss_train + loss\n",
    "            lossall.append(float(loss.cpu()))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return  mean(lossall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 3000/3000 [4:31:44<00:00,  5.43s/it]  \n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled = False\n",
    "loss0 = []\n",
    "for epoch in tqdm(range(3000)):\n",
    "    loss0.append(float(train(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, actuals):\n",
    "    if len(predictions) != len(actuals):\n",
    "        raise ValueError(\"The length of both lists must be the same.\")\n",
    "\n",
    "    # 计算匹配的项数\n",
    "    correct = sum(p == a for p, a in zip(predictions, actuals))\n",
    "    # 计算准确率\n",
    "    accuracy = correct / len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(k):\n",
    "    model.eval()\n",
    "    answer_true = []\n",
    "    answer_test = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(3000,4200)):\n",
    "            for rank0_select in range(0,2):\n",
    "                for rank1_select in range(0,2):\n",
    "                    for  j in range(0,2):\n",
    "                        question = questions_(i, i+7, i+8+k, i+15+k,rank0_select, rank1_select)\n",
    "                        answer_true.append(list(answer_list).index(answers_(i, i+7, i+8+k, i+15+k,rank0_select, rank1_select)[j]))\n",
    "                        output0 = torch.nn.Softmax(dim = 1)(model(taxi_tensor_in, text_s, adj0, i, 8, question[j]).T.cpu())\n",
    "                        output1 = torch.argmax(output0, dim=1).numpy()[0]\n",
    "                        answer_test.append(output1)\n",
    "    return answer_true, answer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_true1, answer_test1 = test_step(k)\n",
    "print(calculate_accuracy(answer_true1, answer_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
